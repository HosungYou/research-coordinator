# Diverga QA Protocol v3.0
# Scenario QUAL-003: Grounded Theory Study with Verification Huddle (English)
#
# Purpose: Test qualitative research agents with grounded theory methodology,
# theoretical sampling, and includes VERIFICATION HUDDLE to confirm real AI responses.
#
# Complexity: MEDIUM (5-7 turns expected)
# Language: English
# Special: Includes API verification markers

scenario_id: QUAL-003
name: "Grounded Theory with Verification Huddle"
version: "3.0"
paradigm: qualitative
complexity_level: MEDIUM

# VERIFICATION HUDDLE: Markers to confirm real AI execution
verification_huddle:
  enabled: true
  markers:
    - type: TIMESTAMP_VARIANCE
      description: "Response timestamps must vary naturally (not fixed intervals)"
    - type: RESPONSE_LENGTH_VARIANCE
      description: "Response lengths must vary (not template-based fixed lengths)"
    - type: CONTEXT_AWARENESS
      description: "AI must reference specific details from user messages"
    - type: NO_SIMULATION_MARKERS
      description: "No [DRY RUN], [SIMULATED], or template markers"
    - type: UNIQUE_SESSION_ID
      description: "Session ID must be unique UUID"
    - type: DYNAMIC_CONTENT
      description: "Content must show reasoning, not pre-scripted templates"

agents_involved:
  - A1-ResearchQuestionRefiner
  - A5-ParadigmWorldviewAdvisor
  - C2-QualitativeDesignConsultant
  - D1-SamplingStrategyAdvisor
  - E2-QualitativeCodingSpecialist

language: "English"
expected_turns: 5-7
expected_technical_questions: 1
expected_methodological_challenges: 1

checkpoints_expected:
  - id: CP_PARADIGM_SELECTION
    level: RED
    mandatory_halt: true
    vs_options_min: 3

  - id: CP_METHODOLOGY_APPROVAL
    level: RED
    mandatory_halt: true

  - id: CP_SAMPLING_STRATEGY
    level: ORANGE
    mandatory_halt: false

conversation_flow:
  - turn: 1
    user_type: INITIAL_REQUEST
    user: |
      I want to study how software developers experience burnout
      in remote work environments. I'm interested in understanding
      the process they go through - from early warning signs to
      either recovery or leaving the profession. I don't have
      preconceived hypotheses, I want the theory to emerge from data.

    expected_behavior:
      paradigm_detection: qualitative
      keyword_triggers:
        - "experience"
        - "process"
        - "theory to emerge"
        - "no preconceived hypotheses"
      checkpoint: CP_PARADIGM_SELECTION
      language: English
      halt: true
      vs_options_min: 3
      options_include:
        - "Grounded Theory"
        - "Classic Grounded Theory (Glaser)"
        - "Constructivist Grounded Theory (Charmaz)"

  - turn: 2
    user_type: METHODOLOGICAL_CHALLENGE
    user: |
      You mentioned Glaser vs Charmaz. What's the practical difference?
      I've heard Glaser is more purist about letting theory emerge,
      while Charmaz acknowledges researcher influence. Which approach
      would be better for my study on developer burnout?

    expected_behavior:
      maintains_checkpoint: true
      provides_explanation: true
      contrasts_approaches:
        - "Glaser: theory discovery, researcher as neutral"
        - "Charmaz: theory construction, researcher reflexivity"
        - "Strauss-Corbin: structured coding paradigm"
      gives_recommendation: true
      waits_for_selection: true
      language: English

  - turn: 3
    user_type: SELECTION
    user: "[B] Constructivist Grounded Theory (Charmaz)"

    expected_behavior:
      logs_selection: true
      agent_maintains: C2-QualitativeDesignConsultant
      next_checkpoint: CP_SAMPLING_STRATEGY
      language: English

  - turn: 4
    user_type: PRACTICAL_CONSTRAINT
    user: |
      How should I do sampling? I know grounded theory uses
      theoretical sampling, but how do I start when I don't
      have a theory yet? And how many participants do I need?

    expected_behavior:
      agent_involvement: D1-SamplingStrategyAdvisor
      explains:
        - "initial purposive sampling"
        - "theoretical sampling after initial coding"
        - "theoretical saturation"
      provides_guidance: true
      checkpoint: CP_SAMPLING_STRATEGY
      language: English

  - turn: 5
    user_type: APPROVAL
    user: |
      This makes sense. I'll start with 8-10 developers from
      different companies and then do theoretical sampling
      based on emerging categories. Please summarize the
      methodology and give me next steps.

    expected_behavior:
      final_approval: true
      generates_summary: true
      suggests_next_steps:
        - "Develop interview protocol"
        - "Begin initial sampling"
        - "Start open coding"
        - "Memo writing"
      language: English
      checkpoint: CP_METHODOLOGY_APPROVAL

validation_rules:
  checkpoint_compliance:
    target: 100%
    red_checkpoints_must_halt: true

  language_consistency:
    input_language: English
    response_language: English
    must_match: true

  grounded_theory_accuracy:
    must_distinguish_approaches: true
    must_explain_theoretical_sampling: true
    must_mention_saturation: true

  verification_huddle:
    no_dry_run_markers: true
    no_simulation_markers: true
    response_variance_required: true
    context_references_required: true

metrics_targets:
  total_turns: ">=5"
  technical_questions_handled: ">=1"
  methodological_challenges_handled: ">=1"
  checkpoint_compliance: "100%"
  language_consistency: "100%"
  verification_huddle_pass: true
