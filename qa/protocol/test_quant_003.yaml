# Diverga QA Protocol v3.0
# Scenario QUANT-003: Meta-Analysis Effect Size Extraction (English)
#
# Purpose: Test meta-analysis agents (C5/C6/C7) with effect size extraction,
# Hedges' g calculation, and heterogeneity analysis using Codex CLI.
#
# Complexity: HIGH (6-8 turns expected)
# Language: English
# CLI: Codex (with gpt-5.2-codex model)

scenario_id: QUANT-003
name: "Meta-Analysis Effect Size Extraction"
version: "3.0"
paradigm: quantitative
complexity_level: HIGH

# VERIFICATION HUDDLE
verification_huddle:
  enabled: true
  markers:
    - type: NO_SIMULATION_MARKERS
    - type: RESPONSE_LENGTH_VARIANCE
    - type: TIMESTAMP_VARIANCE
    - type: CONTEXT_AWARENESS
    - type: UNIQUE_SESSION_ID
    - type: DYNAMIC_CONTENT

agents_involved:
  - A1-ResearchQuestionRefiner
  - B1-SystematicLiteratureScout
  - B3-EffectSizeExtractor
  - C5-MetaAnalysisMaster
  - C6-DataIntegrityGuard
  - C7-ErrorPreventionEngine
  - E1-QuantitativeAnalysisGuide

language: "English"
expected_turns: 6-8
expected_technical_questions: 2
expected_methodological_challenges: 1

checkpoints_expected:
  - id: CP_RESEARCH_DIRECTION
    level: RED
    mandatory_halt: true

  - id: CP_EFFECT_SIZE_SELECTION
    level: RED
    mandatory_halt: true

  - id: CP_HETEROGENEITY_ANALYSIS
    level: ORANGE
    mandatory_halt: false

  - id: CP_METHODOLOGY_APPROVAL
    level: RED
    mandatory_halt: true

conversation_flow:
  - turn: 1
    user_type: INITIAL_REQUEST
    user: |
      I want to conduct a meta-analysis on the effectiveness of AI tutoring
      systems on student learning outcomes. I have 25 studies with various
      effect sizes reported (Cohen's d, Hedges' g, correlation r, and some
      only report means/SDs). How should I standardize these effect sizes?

    expected_behavior:
      paradigm_detection: quantitative
      keyword_triggers:
        - "meta-analysis"
        - "effect sizes"
        - "Cohen's d"
        - "Hedges' g"
        - "standardize"
      checkpoint: CP_RESEARCH_DIRECTION
      language: English
      halt: true
      vs_options_min: 2
      discusses:
        - "effect size conversion"
        - "Hedges' g vs Cohen's d"
        - "sample size bias correction"

  - turn: 2
    user_type: TECHNICAL_FOLLOW_UP
    user: |
      Why is Hedges' g preferred over Cohen's d for meta-analysis? Some of
      my studies have small samples (n < 20). Also, one study only reports
      F-statistics from ANOVA. How do I convert that to Hedges' g?

    expected_behavior:
      provides_formula: true
      discusses:
        - "small sample bias correction"
        - "Hedges' g formula: g = d * J"
        - "J correction factor"
        - "F to d conversion"
        - "degrees of freedom"
      checkpoint: CP_EFFECT_SIZE_SELECTION
      language: English

  - turn: 3
    user_type: METHODOLOGICAL_CHALLENGE
    user: |
      I've extracted the effect sizes. The Q-statistic shows significant
      heterogeneity (Q = 89.34, df = 24, p < .001). I2 is 73%. Should I
      use random effects model? And how do I explore the heterogeneity -
      moderator analysis or meta-regression?

    expected_behavior:
      honest_assessment: true
      discusses:
        - "Q-statistic interpretation"
        - "I2 interpretation (73% = substantial)"
        - "random effects model"
        - "moderator analysis vs meta-regression"
        - "subgroup analysis"
      provides_recommendation: true
      checkpoint: CP_HETEROGENEITY_ANALYSIS
      language: English

  - turn: 4
    user_type: TECHNICAL_FOLLOW_UP
    user: |
      For moderator analysis, I want to test: (1) tutoring modality
      (text-based vs voice-based vs multimodal), (2) subject domain
      (STEM vs humanities), and (3) study quality (low vs high RoB).
      How many subgroups can I test without inflating Type I error?

    expected_behavior:
      provides_guidance: true
      discusses:
        - "multiple comparisons problem"
        - "Bonferroni correction"
        - "a priori vs post-hoc moderators"
        - "minimum k per subgroup"
        - "power considerations"
      warns_about:
        - "fishing expedition"
        - "HARKing"
      language: English

  - turn: 5
    user_type: PRACTICAL_CONSTRAINT
    user: |
      Some of my studies don't report SDs, only SEs or 95% CIs. Also, two
      studies report pre-post effect sizes without control groups. Should
      I include these single-group studies or exclude them?

    expected_behavior:
      provides_calculation: true
      discusses:
        - "SE to SD conversion"
        - "CI to SD conversion"
        - "pre-post vs between-groups design"
        - "single-group study limitations"
        - "sensitivity analysis with/without"
      recommends:
        - "sensitivity analysis"
        - "leave-one-out analysis"
      language: English

  - turn: 6
    user_type: APPROVAL
    user: |
      This is very helpful. I'll use Hedges' g for all effect sizes, random
      effects model with REML estimation, and run moderator analysis for the
      3 a priori moderators with Bonferroni correction. I'll also conduct
      sensitivity analysis excluding single-group studies. Please summarize
      my analysis plan and suggest R packages to use.

    expected_behavior:
      final_approval: true
      generates_summary: true
      suggests_tools:
        - "metafor package"
        - "meta package"
        - "dmetar package"
      suggests_next_steps:
        - "Forest plot generation"
        - "Funnel plot for publication bias"
        - "Egger's test"
        - "Trim-and-fill analysis"
        - "GOSH plot for outliers"
      checkpoint: CP_METHODOLOGY_APPROVAL
      language: English

validation_rules:
  checkpoint_compliance:
    target: 100%
    red_checkpoints_must_halt: true

  language_consistency:
    input_language: English
    response_language: English
    must_match: true

  quantitative_accuracy:
    must_explain_hedges_g: true
    must_discuss_heterogeneity: true
    must_recommend_random_effects: true
    must_address_moderator_analysis: true

  verification_huddle:
    no_dry_run_markers: true
    no_simulation_markers: true
    response_variance_required: true

metrics_targets:
  total_turns: ">=6"
  technical_questions_handled: ">=2"
  checkpoint_compliance: "100%"
  language_consistency: "100%"
  verification_huddle_pass: true

# Model Requirements
# This scenario tests the gpt-5.2-codex model's ability to:
# 1. Understand complex statistical concepts
# 2. Provide accurate effect size conversion formulas
# 3. Give nuanced methodological guidance
# 4. Recommend appropriate analysis tools
