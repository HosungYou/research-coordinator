# Diverga QA Framework Implementation Summary

**Date**: 2026-01-29
**Version**: 1.0.0

---

## λ©μ  (Purpose)

Diverga ν”λ¬κ·ΈμΈμ μ•„ν‚¤ν…μ² λ° νμ΄ν”„λΌμΈμ„ μΆ…ν•©μ μΌλ΅ κ²€μ¦ν•κΈ° μ„ν• QA ν”„λ μ„μ›ν¬ κµ¬ν„:

1. **μ²΄ν¬ν¬μΈνΈ μ‹μ¤ν…** - π”΄/π /π΅ λ λ²¨λ³„ μ •ν™•ν• HALT λ™μ‘ κ²€μ¦
2. **μ—μ΄μ „νΈ νΈμ¶** - 40κ° μ—μ΄μ „νΈμ μ μ ν• νΈλ¦¬κ±° λ° λ¨λΈ ν‹°μ–΄ κ²€μ¦
3. **μ½”λ””λ„¤μ΄ν„°/μ¤μΌ€μ¤νΈλ μ΄ν„°** - μ›ν¬ν”λ΅μ° κ΄€λ¦¬ κ²€μ¦
4. **VS Methodology** - T-Score κΈ°λ° λ€μ• μ μ‹ ν’μ§ κ²€μ¦

---

## κµ¬ν„λ νμΌ κµ¬μ΅°

```
/Volumes/External SSD/Projects/Research/Diverga/qa/
β”β”€β”€ __init__.py                          # ν¨ν‚¤μ§€ μ΄κΈ°ν™” (15 lines)
β”β”€β”€ run_tests.py                         # λ©”μΈ ν…μ¤νΈ λ¬λ„ CLI (280 lines)
β”β”€β”€ requirements.txt                     # μμ΅΄μ„± (pyyaml)
β”β”€β”€ README.md                            # μ‚¬μ©λ²• λ¬Έμ„ (300 lines)
β”β”€β”€ IMPLEMENTATION_SUMMARY.md            # μ΄ λ¬Έμ„
β”‚
β”β”€β”€ protocol/                            # ν…μ¤νΈ μ •μ λ¨λ“
β”‚   β”β”€β”€ __init__.py                      # λ¨λ“ exports (30 lines)
β”‚   β”β”€β”€ scenarios.py                     # μ‹λ‚λ¦¬μ¤ ν΄λμ¤ μ •μ (280 lines)
β”‚   β”β”€β”€ metrics.py                       # ν‰κ°€ λ©”νΈλ¦­ ν΄λμ¤ (350 lines)
β”‚   β”β”€β”€ test_meta_001.yaml               # λ©”νƒ€λ¶„μ„ μ‹λ‚λ¦¬μ¤ (140 lines)
β”‚   β”β”€β”€ test_qual_001.yaml               # μ§μ μ—°κµ¬ μ‹λ‚λ¦¬μ¤ (130 lines)
β”‚   β”β”€β”€ test_mixed_001.yaml              # νΌν•©λ°©λ²• μ‹λ‚λ¦¬μ¤ (130 lines)
β”‚   β””β”€β”€ test_human_001.yaml              # ν΄λ¨Όν™” μ‹λ‚λ¦¬μ¤ (120 lines)
β”‚
β”β”€β”€ runners/                             # μ‹¤ν–‰ μ—”μ§„ λ¨λ“
β”‚   β”β”€β”€ __init__.py                      # λ¨λ“ exports (15 lines)
β”‚   β”β”€β”€ conversation_simulator.py        # λ€ν™” μ‹λ®¬λ μ΄ν„° (300 lines)
β”‚   β”β”€β”€ checkpoint_validator.py          # μ²΄ν¬ν¬μΈνΈ κ²€μ¦κΈ° (250 lines)
β”‚   β””β”€β”€ agent_tracker.py                 # μ—μ΄μ „νΈ μ¶”μ κΈ° (280 lines)
β”‚
β””β”€β”€ reports/                             # ν…μ¤νΈ κ²°κ³Ό μ €μ¥
    β””β”€β”€ .gitkeep
```

**μ΄ μ½”λ“λ‰**: ~2,300 lines (Python + YAML + Markdown)

---

## ν•µμ‹¬ μ»΄ν¬λ„νΈ

### 1. Scenario System (`protocol/scenarios.py`)

```python
@dataclass
class Scenario:
    scenario_id: str          # e.g., "META-001"
    name: str                 # μ‹λ‚λ¦¬μ¤ μ΄λ¦„
    paradigm: Paradigm        # quantitative | qualitative | mixed_methods
    priority: Priority        # critical | high | medium | low

    agents_primary: list[AgentExpectation]
    checkpoints_required: list[CheckpointExpectation]
    conversation_flow: list[ConversationTurn]
```

### 2. Metrics System (`protocol/metrics.py`)

| ν΄λμ¤ | λ©μ  | μ£Όμ” λ©”νΈλ¦­ |
|--------|------|------------|
| `CheckpointMetrics` | μ²΄ν¬ν¬μΈνΈ κ²€μ¦ | halt_verified, vs_options_count, t_scores_shown |
| `AgentMetrics` | μ—μ΄μ „νΈ μ¶”μ  | invoked, correct_model_tier, response_grade |
| `VSQualityMetrics` | VS ν’μ§ ν‰κ°€ | t_score_spread, modal_avoidance, creative_options |
| `TestResult` | μΆ…ν•© κ²°κ³Ό | overall_score, grade (A-F), issues/warnings |

### 3. Checkpoint Validator (`runners/checkpoint_validator.py`)

```python
class CheckpointValidator:
    REQUIRED_CHECKPOINTS = [
        "CP_RESEARCH_DIRECTION",
        "CP_PARADIGM_SELECTION",
        "CP_THEORY_SELECTION",
        "CP_METHODOLOGY_APPROVAL",
    ]

    def validate(self, response, checkpoint_id, level) -> ValidationResult:
        # 1. μ²΄ν¬ν¬μΈνΈ νΈλ¦¬κ±° κ°μ§€
        # 2. HALT λ™μ‘ ν™•μΈ
        # 3. VS λ€μ• μ μ‹ ν™•μΈ
        # 4. T-Score ν‘μ‹ ν™•μΈ
        # 5. λ…μ‹μ  λ€κΈ° ν™•μΈ
```

### 4. Agent Tracker (`runners/agent_tracker.py`)

```python
class AgentTracker:
    # 40κ° μ—μ΄μ „νΈ ν‚¤μ›λ“ λ§¤ν•‘
    AGENT_KEYWORDS = {
        "diverga:c5": ["meta-analysis", "λ©”νƒ€λ¶„μ„", ...],
        "diverga:a1": ["research question", "μ—°κµ¬ μ§λ¬Έ", ...],
        ...
    }

    # λ¨λΈ ν‹°μ–΄ ν• λ‹Ή
    AGENT_TIERS = {
        "diverga:c5": "opus",    # HIGH
        "diverga:c6": "sonnet",  # MEDIUM
        "diverga:b3": "haiku",   # LOW
    }
```

---

## ν…μ¤νΈ μ‹λ‚λ¦¬μ¤

### META-001: Meta-Analysis Pipeline (Critical)

| ν•­λ© | κ°’ |
|------|-----|
| **ν¨λ¬λ‹¤μ„** | Quantitative |
| **μ£Όμ” μ—μ΄μ „νΈ** | C5-MetaAnalysisMaster |
| **μ²΄ν¬ν¬μΈνΈ** | CP_RESEARCH_DIRECTION, CP_METHODOLOGY_APPROVAL |
| **λ€ν™” ν„΄** | 3 turns |

```yaml
conversation_flow:
  - turn: 1
    user: "I want to conduct a meta-analysis on AI tutors..."
    expected: CP_RESEARCH_DIRECTION νΈλ¦¬κ±°, VS λ€μ• 3κ°

  - turn: 2
    user: "[B] Subject-specific effects"
    expected: C5 μ—μ΄μ „νΈ νΈμ¶, CP_METHODOLOGY_APPROVAL νΈλ¦¬κ±°

  - turn: 3
    user: "Yes, approve the methodology"
    expected: C6 μ—μ΄μ „νΈ νΈμ¶, μ§„ν–‰
```

### QUAL-001: Phenomenology Study (High)

| ν•­λ© | κ°’ |
|------|-----|
| **ν¨λ¬λ‹¤μ„** | Qualitative |
| **μ£Όμ” μ—μ΄μ „νΈ** | C2-QualitativeDesignConsultant |
| **μ²΄ν¬ν¬μΈνΈ** | CP_PARADIGM_SELECTION, CP_THEORY_SELECTION, CP_METHODOLOGY_APPROVAL |

### MIXED-001: Sequential Explanatory (High)

| ν•­λ© | κ°’ |
|------|-----|
| **ν¨λ¬λ‹¤μ„** | Mixed Methods |
| **μ£Όμ” μ—μ΄μ „νΈ** | C3-MixedMethodsDesignConsultant, E3-MixedMethodsIntegration |
| **μ²΄ν¬ν¬μΈνΈ** | CP_RESEARCH_DIRECTION, CP_INTEGRATION_STRATEGY |

### HUMAN-001: Humanization Pipeline (High)

| ν•­λ© | κ°’ |
|------|-----|
| **ν¨λ¬λ‹¤μ„** | Any |
| **μ£Όμ” μ—μ΄μ „νΈ** | G5-AcademicStyleAuditor, G6-AcademicStyleHumanizer, F5-HumanizationVerifier |
| **μ²΄ν¬ν¬μΈνΈ** | CP_HUMANIZATION_REVIEW (RECOMMENDED) |

---

## ν‰κ°€ κΈ°μ¤€

### κ°€μ¤‘μΉ λ°°λ¶„

| μΉ΄ν…κ³ λ¦¬ | κ°€μ¤‘μΉ | μ„¤λ… |
|----------|--------|------|
| Checkpoint Compliance | 40% | HALT, wait, VS λ€μ• |
| Agent Accuracy | 35% | μ¬λ°”λ¥Έ μ—μ΄μ „νΈ, μ¬λ°”λ¥Έ ν‹°μ–΄ |
| VS Quality | 25% | T-Score λ‹¤μ–‘μ„±, modal νν”Ό |

### λ“±κΈ‰ κΈ°μ¤€

| Grade | μ μ | κΈ°μ¤€ |
|-------|------|------|
| **A** | β‰¥90 | μ™„λ²½ν• μ²΄ν¬ν¬μΈνΈ, μ¬λ°”λ¥Έ μ—μ΄μ „νΈ, VS λ€μ• + T-Score |
| **B** | β‰¥80 | μ¬λ°”λ¥Έ μ—μ΄μ „νΈ, μ²΄ν¬ν¬μΈνΈ μ‘λ™, μ‚¬μ†ν• gaps |
| **C** | β‰¥70 | μ¬λ°”λ¥Έ μ—μ΄μ „νΈ, μ²΄ν¬ν¬μΈνΈ μμΌλ‚ μ•½ν• VS |
| **D** | β‰¥60 | μλ»λ μ—μ΄μ „νΈ λλ” λ„λ½λ μ²΄ν¬ν¬μΈνΈ |
| **F** | <60 | π”΄ μ²΄ν¬ν¬μΈνΈμ—μ„ μΉμΈ μ—†μ΄ μ§„ν–‰ |

---

## μ‚¬μ©λ²•

### μ„¤μΉ

```bash
cd /Volumes/External\ SSD/Projects/Research/Diverga
pip install -r qa/requirements.txt
```

### μ‹¤ν–‰

```bash
# μ‹λ‚λ¦¬μ¤ λ©λ΅ ν™•μΈ
python -m qa.run_tests --list

# νΉμ • μ‹λ‚λ¦¬μ¤ μ‹¤ν–‰
python -m qa.run_tests --scenario META-001 --verbose

# λ¨λ“  μ‹λ‚λ¦¬μ¤ μ‹¤ν–‰
python -m qa.run_tests --all --report json

# νΉμ • μ²΄ν¬ν¬μΈνΈλ§ ν…μ¤νΈ
python -m qa.run_tests --checkpoint CP_RESEARCH_DIRECTION
```

### ν”„λ΅κ·Έλλ§¤ν‹± μ‚¬μ©

```python
from qa.protocol.scenarios import load_scenario
from qa.runners.conversation_simulator import ConversationSimulator

# μ‹λ‚λ¦¬μ¤ λ΅λ“
scenario = load_scenario("META-001")

# μ‹λ®¬λ μ΄ν„° μƒμ„±
simulator = ConversationSimulator(scenario)

# λ€ν™” μ‹¤ν–‰
result = simulator.run_turn(user_input, ai_response)

# κ²°κ³Ό ν™•μΈ
print(f"Passed: {result.passed}")
print(f"Issues: {result.issues}")

# μµμΆ… λ¦¬ν¬νΈ
test_result = simulator.finalize()
test_result.to_yaml("report.yaml")
```

---

## ν–¥ν›„ ν™•μ¥ κ³„ν

1. **μ‹¤μ  AI μ‘λ‹µ ν…μ¤νΈ**: Claude API μ—°λ™ν•μ—¬ μ‹¤μ  μ‘λ‹µ κ²€μ¦
2. **CI/CD ν†µν•©**: GitHub Actions μ›ν¬ν”λ΅μ° μ¶”κ°€
3. **μ¶”κ°€ μ‹λ‚λ¦¬μ¤**: μ—£μ§€ μΌ€μ΄μ¤, μ—λ¬ ν•Έλ“¤λ§ μ‹λ‚λ¦¬μ¤
4. **μ»¤λ²„λ¦¬μ§€ λ¦¬ν¬νΈ**: ν…μ¤νΈλ μ—μ΄μ „νΈ/μ²΄ν¬ν¬μΈνΈ μ»¤λ²„λ¦¬μ§€

---

## μ°Έμ΅° νμΌ

- Coordinator: `.claude/skills/research-coordinator/SKILL.md`
- Orchestrator: `.claude/skills/research-orchestrator/SKILL.md`
- Checkpoints: `.claude/skills/research-coordinator/interaction/user-checkpoints.md`
- Agents: `/Volumes/External SSD/Projects/Research/Diverga/agents/`
