# Diverga QA Protocol v3.0

**True Automated Testing via CLI**

## Overview

Diverga QA Protocol v3.0ì€ ì‹¤ì œ AI ì‘ë‹µì„ CLI ë„êµ¬ë¥¼ í†µí•´ ìë™ìœ¼ë¡œ ìº¡ì²˜í•˜ëŠ” í…ŒìŠ¤íŠ¸ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### v2.x vs v3.0 ë¹„êµ

| í•­ëª© | v2.x (ì‹œë®¬ë ˆì´ì…˜) | v3.0 (ì§„ì •í•œ ìë™í™”) |
|------|------------------|---------------------|
| **AI ì‘ë‹µ** | `RESPONSE_TEMPLATES` dict | **ì‹¤ì œ AI ìƒì„± ì‘ë‹µ** |
| **ì‹¤í–‰ ë°©ì‹** | Python ì‹œë®¬ë ˆì´í„° | **CLI ë¹„ëŒ€í™”í˜• ëª¨ë“œ** |
| **ê²€ì¦ ê°€ì¹˜** | í”„ë¡œí† ì½œ í˜•ì‹ë§Œ | **ì‹¤ì œ ê¸°ëŠ¥ ê²€ì¦** |
| **API í˜¸ì¶œ** | ì—†ìŒ | ì‹¤ì œ í† í° ì†Œë¹„ |

---

## Quick Start

### ë‹¨ì¼ ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰ (v3.0 ê¶Œì¥)

```bash
# ì‹¤ì œ AI í…ŒìŠ¤íŠ¸
python3 qa/runners/cli_test_runner.py --scenario QUAL-002 --cli claude

# Dry Run (API í˜¸ì¶œ ì—†ìŒ)
python3 qa/runners/cli_test_runner.py --scenario QUAL-002 --dry-run

# Verbose ëª¨ë“œ
python3 qa/runners/cli_test_runner.py --scenario QUAL-002 -v
```

### ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ ì‹¤í–‰

```bash
# ì‹¤ì œ AIë¡œ ëª¨ë“  ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸
./qa/run_all_scenarios.sh

# Dry Run ëª¨ë“œ
./qa/run_all_scenarios.sh --dry-run

# ë‹¤ë¥¸ CLI ë„êµ¬ ì‚¬ìš©
./qa/run_all_scenarios.sh --cli opencode
```

### v2.x ì‹œë®¬ë ˆì´ì…˜ (Legacy)

```bash
# ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ (í•˜ë“œì½”ë”©ëœ ì‘ë‹µ)
python3 qa/runners/automated_test.py --scenario QUAL-002
```

---

## ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
qa/
â”œâ”€â”€ README.md                    # ì´ ë¬¸ì„œ
â”œâ”€â”€ run_all_scenarios.sh         # v3.0 ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ run_tests.py                 # v2.x í…ŒìŠ¤íŠ¸ ëŸ¬ë„ˆ
â”‚
â”œâ”€â”€ protocol/                    # í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
â”‚   â”œâ”€â”€ test_qual_001.yaml       # ê¸°ë³¸ ì§ˆì  ì—°êµ¬
â”‚   â”œâ”€â”€ test_qual_002.yaml       # ê³ ê¸‰ í˜„ìƒí•™ (í•œêµ­ì–´)
â”‚   â”œâ”€â”€ test_meta_001.yaml       # ê¸°ë³¸ ë©”íƒ€ë¶„ì„
â”‚   â”œâ”€â”€ test_meta_002.yaml       # ê³ ê¸‰ ë©”íƒ€ë¶„ì„ (ì˜ì–´)
â”‚   â”œâ”€â”€ test_mixed_001.yaml      # í˜¼í•©ë°©ë²•
â”‚   â”œâ”€â”€ test_mixed_002.yaml      # ê³ ê¸‰ í˜¼í•©ë°©ë²•
â”‚   â”œâ”€â”€ test_human_001.yaml      # ì¸ê°„ ì²´í¬í¬ì¸íŠ¸
â”‚   â””â”€â”€ test_human_002.yaml      # ê³ ê¸‰ ì²´í¬í¬ì¸íŠ¸
â”‚
â”œâ”€â”€ runners/                     # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ê¸°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ cli_test_runner.py       # v3.0 CLI ê¸°ë°˜ ìë™í™” (NEW)
â”‚   â”œâ”€â”€ automated_test.py        # v2.x ì‹œë®¬ë ˆì´í„°
â”‚   â”œâ”€â”€ extract_conversation.py  # JSONL ì„¸ì…˜ íŒŒì‹±
â”‚   â”œâ”€â”€ checkpoint_validator.py  # ì²´í¬í¬ì¸íŠ¸ ê²€ì¦
â”‚   â””â”€â”€ agent_tracker.py         # ì—ì´ì „íŠ¸ ì¶”ì 
â”‚
â””â”€â”€ reports/                     # í…ŒìŠ¤íŠ¸ ê²°ê³¼
    â”œâ”€â”€ sessions/                # ì„¸ì…˜ë³„ ê²°ê³¼
    â”‚   â””â”€â”€ QUAL-002/
    â”‚       â”œâ”€â”€ README.md
    â”‚       â”œâ”€â”€ conversation_transcript.md
    â”‚       â”œâ”€â”€ conversation_raw.json
    â”‚       â””â”€â”€ QUAL-002_test_result.yaml
    â””â”€â”€ real-transcripts/        # ì‹¤ì œ ëŒ€í™” ê¸°ë¡
```

---

## CLI Test Runner (v3.0)

### CLITestRunner í´ë˜ìŠ¤

```python
from qa.runners import CLITestRunner

runner = CLITestRunner(
    scenario_id='QUAL-002',      # ì‹œë‚˜ë¦¬ì˜¤ ID
    cli_tool='claude',           # CLI ë„êµ¬ (claude, opencode, codex)
    verbose=True,                # ìƒì„¸ ì¶œë ¥
    dry_run=False,               # Dry Run ëª¨ë“œ
    timeout=300                  # í„´ë‹¹ íƒ€ì„ì•„ì›ƒ (ì´ˆ)
)

session = runner.run()
runner.save_results('qa/reports/sessions')
```

### ì§€ì› CLI ë„êµ¬

| CLI | ëª…ë ¹ | ì„¸ì…˜ ì§€ì† |
|-----|------|----------|
| `claude` | `claude -p "message"` | `--continue` |
| `opencode` | `opencode run "message"` | - |
| `codex` | `codex exec "message"` | `--resume` |

### ì¶œë ¥ íŒŒì¼

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `README.md` | ì„¸ì…˜ ê°œìš” ë° ë©”íŠ¸ë¦­ |
| `conversation_transcript.md` | ì‹¤ì œ AI ì‘ë‹µ í¬í•¨ ëŒ€í™” ê¸°ë¡ |
| `conversation_raw.json` | ë©”íƒ€ë°ì´í„° í¬í•¨ RAW ë°ì´í„° |
| `{SCENARIO}_test_result.yaml` | í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë° ê²€ì¦ |

---

## í…ŒìŠ¤íŠ¸ ì‹œë‚˜ë¦¬ì˜¤

### QUAL-002: ê³ ê¸‰ í˜„ìƒí•™ (í•œêµ­ì–´)

```yaml
scenario_id: QUAL-002
name: "Advanced Phenomenology with Paradigm Debates"
paradigm: qualitative
complexity_level: HIGH
language: "Korean (user input) -> Korean (response)"
expected_turns: 8-12

checkpoints_expected:
  - CP_PARADIGM_SELECTION (RED)
  - CP_METHODOLOGY_APPROVAL (RED)
  - CP_PARADIGM_RECONSIDERATION (ORANGE)
  - CP_ANALYSIS_APPROACH (ORANGE)

agents_involved:
  - A1-ResearchQuestionRefiner
  - A5-ParadigmWorldviewAdvisor
  - C2-QualitativeDesignConsultant
  - D2-InterviewFocusGroupSpecialist
  - E2-QualitativeCodingSpecialist
  - A3-DevilsAdvocate
```

### META-002: ê³ ê¸‰ ë©”íƒ€ë¶„ì„ (ì˜ì–´)

```yaml
scenario_id: META-002
name: "Advanced Meta-Analysis with Theoretical Debates"
paradigm: quantitative
language: English
expected_turns: 8-12
```

### MIXED-002: í˜¼í•©ë°©ë²•

```yaml
scenario_id: MIXED-002
paradigm: mixed
language: English
expected_turns: 8-10
```

### HUMAN-002: í•™ìˆ  íœ´ë¨¼í™”

```yaml
scenario_id: HUMAN-002
paradigm: qualitative
language: English
expected_turns: 6-8
```

---

## ê²€ì¦ ë©”íŠ¸ë¦­

### ì²´í¬í¬ì¸íŠ¸ íƒì§€

```python
# ì²´í¬í¬ì¸íŠ¸ íŒ¨í„´
patterns = [
    r'ğŸ”´\s*CHECKPOINT[:\s]+(\w+)',   # RED
    r'ğŸŸ \s*CHECKPOINT[:\s]+(\w+)',   # ORANGE
    r'ğŸŸ¡\s*CHECKPOINT[:\s]+(\w+)',   # YELLOW
    r'CHECKPOINT[:\s]+(CP_\w+)',
]
```

### ì—ì´ì „íŠ¸ íƒì§€

```python
# ì—ì´ì „íŠ¸ ì°¸ì¡° íŒ¨í„´
patterns = [
    r'diverga:([a-z]\d+)',           # diverga:a1
    r'([A-Z]\d+)-\w+',               # A1-ResearchQuestionRefiner
    r'Task.*subagent_type.*diverga:(\w+)',
]
```

### VS ì˜µì…˜ ì¶”ì¶œ

```python
# T-Score í¬í•¨ ì˜µì…˜
pattern = r'\[([A-Z])\]\s*([^(]+?)\s*\(T\s*=\s*(\d+\.?\d*)\)'
# ê²°ê³¼: {'option': 'B', 'label': 'í•´ì„í•™ì  í˜„ìƒí•™', 't_score': 0.40}
```

---

## í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì˜ˆì‹œ

### QUAL-002 ì‹¤í–‰ ê²°ê³¼ (2026-01-29)

```
============================================================
Diverga QA Protocol v3.0 - True Automated Testing
Scenario: QUAL-002
CLI Tool: claude
Mode: LIVE
============================================================

[Turn 1] INITIAL_REQUEST
  Received: 792 chars
  âœ“ Completed (CP: 1, Agents: 0)

[Turn 2] METHODOLOGICAL_CHALLENGE
  Received: 1810 chars
  âœ“ Completed (CP: 1, Agents: 0)

[Turn 3] SELECTION
  Received: 2469 chars
  âœ“ Completed (CP: 1, Agents: 0)

[Turn 4] ALTERNATIVE_EXPLORATION
  Received: 3348 chars
  âœ“ Completed (CP: 1, Agents: 0)

[Turn 5] PRACTICAL_CONSTRAINT
  Received: 2966 chars
  âœ“ Completed (CP: 1, Agents: 0)

[Turn 6] PARADIGM_QUESTIONING
  Received: 3315 chars
  âœ“ Completed (CP: 1, Agents: 0)

[Turn 7] SELECTION
  Received: 5327 chars
  âœ“ Completed (CP: 1, Agents: 0)

[Turn 8] APPROVAL
  Received: 889 chars
  âœ“ Completed (CP: 1, Agents: 0)

============================================================
Test Completed: QUAL-002
Turns: 8
Checkpoints: 8
============================================================
```

### ë©”íŠ¸ë¦­ ìš”ì•½

| ë©”íŠ¸ë¦­ | ê°’ |
|--------|-----|
| Total Turns | 8 |
| Checkpoints Found | 8 |
| Total Response Chars | ~21,000 |
| Test Duration | ~4 minutes |

---

## User Input Types

| Type | Description | Example |
|------|-------------|---------|
| `INITIAL_REQUEST` | ì—°êµ¬ ì£¼ì œ ì œì‹œ | "êµì‚¬ë“¤ì´ AI ë„êµ¬ë¥¼ ê²½í—˜í•˜ëŠ” í˜„ìƒì„ íƒêµ¬í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤" |
| `TECHNICAL_FOLLOW_UP` | ê¸°ìˆ ì  ì§ˆë¬¸ | "Husserlì˜ bracketê³¼ Heideggerì˜ hermeneutic circle ì°¨ì´ëŠ”?" |
| `METHODOLOGICAL_CHALLENGE` | ë°©ë²•ë¡ ì  ë„ì „ | "ì™œ IPA ëŒ€ì‹  van Manenì¸ê°€ìš”?" |
| `SELECTION` | ì˜µì…˜ ì„ íƒ | "[B] í•´ì„í•™ì  í˜„ìƒí•™ (van Manen)" |
| `PRACTICAL_CONSTRAINT` | í˜„ì‹¤ì  ì œì•½ | "ì°¸ì—¬ìê°€ 5ëª…ë°–ì— ì•ˆ ë˜ëŠ”ë° ì¶©ë¶„í• ê¹Œìš”?" |
| `PARADIGM_QUESTIONING` | íŒ¨ëŸ¬ë‹¤ì„ ì¬ê³  | "í˜¼í•© ë°©ë²•ìœ¼ë¡œ ê°€ëŠ” ê²Œ ë” ë‚˜ì„ê¹Œìš”?" |
| `APPROVAL` | ìŠ¹ì¸ | "ìŠ¹ì¸í•©ë‹ˆë‹¤. ì´ ë°©ë²•ë¡ ìœ¼ë¡œ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤." |

---

## Checkpoint Levels

| Level | Symbol | Behavior |
|-------|--------|----------|
| RED | ğŸ”´ | MUST HALT, wait for approval |
| ORANGE | ğŸŸ  | SHOULD HALT |
| YELLOW | ğŸŸ¡ | MAY proceed |

---

## ë¬¸ì œ í•´ê²°

### CLI ë„êµ¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ

```bash
# Claude Code ì„¤ì¹˜ í™•ì¸
which claude

# ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš°
npm install -g @anthropic-ai/claude-code
```

### íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜

```bash
# íƒ€ì„ì•„ì›ƒ ì¦ê°€ (10ë¶„)
python3 qa/runners/cli_test_runner.py --scenario QUAL-002 --timeout 600
```

---

## Changelog

### v3.0 (2026-01-29)
- **True automated testing via CLI** - ì‹¤ì œ AI ì‘ë‹µ ìº¡ì²˜
- **CLITestRunner í´ë˜ìŠ¤** - subprocess ê¸°ë°˜ CLI ì‹¤í–‰
- **Multi-turn ì„¸ì…˜ ì§€ì›** - `--continue` í”Œë˜ê·¸ë¡œ ëŒ€í™” ì§€ì†
- **Dry run ëª¨ë“œ** - API í˜¸ì¶œ ì—†ì´ í…ŒìŠ¤íŠ¸ êµ¬ì¡° í™•ì¸
- **run_all_scenarios.sh** - ë°°ì¹˜ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

### v2.2 (2026-01-29)
- **Automated test simulation** - `RESPONSE_TEMPLATES` ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜
- **CLI-based execution** - `python3 qa/runners/automated_test.py`

### v2.1 (2026-01-29)
- **Session-based folder management** - `reports/sessions/{SCENARIO-ID}/`
- **RAW conversation extraction** - `conversation_raw.json`

### v2.0 (2026-01-29)
- Migrated to real Claude Code conversations
- Added complex user input types
- Implemented JSONL session log extraction
